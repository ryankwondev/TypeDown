# Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning

### Abstract

  *Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at rel-atively low computational cost. Recently, the introduction of residual connections in conjunction with a more tradi-tional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any beneﬁt in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks signiﬁcantly. There is also some evidence of residual Incep-tion networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These varia-tions improve the single-frame recognition performance on the ILSVRC 2012 classiﬁcation task signiﬁcantly. We fur-ther demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08% top-5 error on the test set of the ImageNet classiﬁcation (CLS) challenge.*

#### 1. Introduction

  Since the 2012 ImageNet competition [11] winning en-try by Krizhevsky et al [8], their network “AlexNet” has been successfully applied to a larger variety of computer vision tasks, for example to object-detection [4], segmen-tation [10], human pose estimation [17], video classiﬁca-tion [7], object tracking [18], and superresolution [3]. These examples are but a few of all the applications to which deep convolutional networks have been very successfully applied ever since.
  In this work we study the combination of the two most recent ideas: Residual connections introduced by He et al. in [5] and the latest revised version of the Inception archi-tecture [15]. In [5], it is argued that residual connections are of inherent importance for training very deep architectures. Since Inception networks tend to be very deep, it is natu-ral to replace the ﬁlter concatenation stage of the Inception architecture with residual connections. This would allow Inception to reap all the beneﬁts of the residual approach while retaining its computational efﬁciency.
  Besides a straightforward integration, we have also stud-ied whether Inception itself can be made more efﬁcient by making it deeper and wider. For that purpose, we designed a new version named Inception-v4 which has a more uni-form simpliﬁed architecture and more inception modules than Inception-v3. Historically, Inception-v3 had inherited a lot of the baggage of the earlier incarnations. The techni-cal constraints chieﬂy came from the need for partitioning the model for distributed training using DistBelief [2]. Now, after migrating our training setup to TensorFlow [1] these constraints have been lifted, which allowed us to simplify the architecture signiﬁcantly. The details of that simpliﬁed architecture are described in Section 3.
  In this report, we will compare the two pure Inception variants, Inception-v3 and v4, with similarly expensive hy-brid Inception-ResNet versions. Admittedly, those mod-els were picked in a somewhat ad hoc manner with the main constraint being that the parameters and computa-tional complexity of the models should be somewhat similar to the cost of the non-residual models. In fact we have tested bigger and wider Inception-ResNet variants and they per-formed very similarly on the ImageNet classiﬁcation chal-lenge [11] dataset.
  The last experiment reported here is an evaluation of an ensemble of all the best performing models presented here. As it was apparent that both Inception-v4 and Inception-ResNet-v2 performed similarly well, exceeding state-of-the art single frame performance on the ImageNet valida-tion dataset, we wanted to see how a combination of those pushes the state of the art on this well studied dataset. Sur-prisingly, we found that gains on the single-frame perfor-mance do not translate into similarly large gains on ensem-bled performance. Nonetheless, it still allows us to report 3.1% top-5 error on the validation set with four models en-sembled setting a new state of the art, to our best knowl-edge.
  In the last section, we study some of the classiﬁcation failures and conclude that the ensemble still has not reached the label noise of the annotations on this dataset and there is still room for improvement for the predictions.


#### 2. Related Work

  Convolutional networks have become popular in large scale image recognition tasks after Krizhevsky et al. [8]. Some of the next important milestones were Network-in-network [9] by Lin et al., VGGNet [12] by Simonyan et al. and GoogLeNet (Inception-v1) [14] by Szegedy et al.
  Residual connection were introduced by He et al. in [5] in which they give convincing theoretical and practical ev-idence for the advantages of utilizing additive merging of signals both for image recognition, and especially for object detection. The authors argue that residual connections are inherently necessary for training very deep convolutional models. Our ﬁndings do not seem to support this view, at least for image recognition. However it might require more measurement points with deeper architectures to understand the true extent of beneﬁcial aspects offered by residual con-nections. In the experimental section we demonstrate that it is not very difﬁcult to train competitive very deep net-works without utilizing residual connections. However the use of residual connections seems to improve the training speed greatly, which is alone a great argument for their use.
  The Inception deep convolutional architecture was intro-duced in [14] and was called GoogLeNet or Inception-v1 in our exposition. Later the Inception architecture was reﬁned in various ways, ﬁrst by the introduction of batch normaliza-tion [6] (Inception-v2) by Ioffe et al. Later the architecture was improved by additional factorization ideas in the third iteration [15] which will be referred to as Inception-v3 in this report.
  
  ![Figure 1. Residual connections as introduced in He et al. [5].](fig_1.png)
